{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d2f7b12",
   "metadata": {},
   "source": [
    "\n",
    "# ComiCo: Federated Deep Learning Simulation\n",
    "\n",
    "This notebook delivers the clean federated adaptation of the original ComiCo workflow. It follows the requested methodology: simulate heterogeneous network conditions, train local QoE models on each client, aggregate them federatively, and compare with centralized training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87825aa",
   "metadata": {},
   "source": [
    "\n",
    "## Context & Objectives\n",
    "\n",
    "- Extend ComiCo to support multiple concurrent clients and distributed training.\n",
    "- Simulate bandwidth dynamics for household, rural, and vehicular contexts (extensible to additional scenarios).\n",
    "- Implement local learning loops and a central server that applies FedAvg-style aggregation (FedProx-ready).\n",
    "- Analyse how update cadence, client count, and network instability influence convergence and robustness.\n",
    "- Provide reusable utilities to scale from 3 up to 15 clients and drive comparative experiments versus the centralized baseline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcdc4b7",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Initialisation\n",
    "\n",
    "Import dependencies and configure deterministic behaviour for reproducible simulations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b372ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error, mean_squared_error\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader, TensorDataset\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "pd.options.display.float_format = \"{:,.3f}\".format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e905ef",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Network & Playback Simulation\n",
    "\n",
    "We encode bandwidth scenarios and utilities to synthesise per-client traces, including repeated playback loops that enrich local datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48349a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class NetworkScenario:\n",
    "    name: str\n",
    "    base_throughput_kbps: float\n",
    "    throughput_std: float\n",
    "    playback_bitrate_kbps: float\n",
    "    base_latency_ms: float\n",
    "    latency_std: float\n",
    "    base_loss: float\n",
    "    loss_std: float\n",
    "    initial_buffer_s: float\n",
    "    rebuffer_threshold_s: float = 0.5\n",
    "\n",
    "\n",
    "def simulate_network_trace(\n",
    "    scenario: NetworkScenario,\n",
    "    duration_s: int = 600,\n",
    "    step_s: int = 1,\n",
    "    seed: int | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    timesteps = np.arange(0, duration_s, step_s, dtype=float)\n",
    "\n",
    "    throughput = rng.normal(\n",
    "        loc=scenario.base_throughput_kbps,\n",
    "        scale=scenario.throughput_std,\n",
    "        size=timesteps.size,\n",
    "    )\n",
    "    throughput = np.clip(throughput, 200.0, None)\n",
    "\n",
    "    latency = rng.normal(\n",
    "        loc=scenario.base_latency_ms,\n",
    "        scale=scenario.latency_std,\n",
    "        size=timesteps.size,\n",
    "    )\n",
    "    latency = np.clip(latency, 5.0, None)\n",
    "\n",
    "    loss = rng.normal(\n",
    "        loc=scenario.base_loss,\n",
    "        scale=scenario.loss_std,\n",
    "        size=timesteps.size,\n",
    "    )\n",
    "    loss = np.clip(loss, 0.0, 1.0)\n",
    "\n",
    "    buffer_levels: List[float] = []\n",
    "    rebuffer_events: List[float] = []\n",
    "    quality_ratio: List[float] = []\n",
    "    qoe_scores: List[float] = []\n",
    "\n",
    "    buffer_level = scenario.initial_buffer_s\n",
    "\n",
    "    for sample_throughput, sample_latency, sample_loss in zip(throughput, latency, loss):\n",
    "        download_time = scenario.playback_bitrate_kbps / max(sample_throughput, 1.0)\n",
    "        buffer_delta = 1.0 - download_time\n",
    "        buffer_level = max(buffer_level + buffer_delta, 0.0)\n",
    "\n",
    "        rebuffer = float(buffer_level <= scenario.rebuffer_threshold_s)\n",
    "        if rebuffer:\n",
    "            buffer_level = scenario.initial_buffer_s * 0.6\n",
    "\n",
    "        quality_ratio.append(float(np.clip(sample_throughput / scenario.playback_bitrate_kbps, 0.1, 3.0)))\n",
    "        buffer_levels.append(buffer_level)\n",
    "        rebuffer_events.append(rebuffer)\n",
    "\n",
    "        penalty = (\n",
    "            0.5 * (scenario.playback_bitrate_kbps / max(sample_throughput, scenario.playback_bitrate_kbps))\n",
    "            + 0.3 * (sample_latency / (sample_latency + 40.0))\n",
    "            + 0.2 * sample_loss\n",
    "            + 0.35 * rebuffer\n",
    "        )\n",
    "        qoe_scores.append(float(np.clip(1.0 - penalty, 0.0, 1.0)))\n",
    "\n",
    "    trace = pd.DataFrame(\n",
    "        {\n",
    "            \"time_s\": timesteps,\n",
    "            \"throughput_kbps\": throughput,\n",
    "            \"latency_ms\": latency,\n",
    "            \"loss_rate\": loss,\n",
    "            \"buffer_level_s\": buffer_levels,\n",
    "            \"rebuffer_event\": rebuffer_events,\n",
    "            \"quality_ratio\": quality_ratio,\n",
    "            \"qoe_score\": qoe_scores,\n",
    "            \"scenario\": scenario.name,\n",
    "        }\n",
    "    )\n",
    "    return trace\n",
    "\n",
    "\n",
    "def enrich_with_replays(\n",
    "    trace: pd.DataFrame,\n",
    "    repeats: int = 2,\n",
    "    noise: float = 0.08,\n",
    "    seed: int | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    if repeats <= 0:\n",
    "        return trace.copy()\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    augmented = [trace.copy()]\n",
    "    for _ in range(repeats):\n",
    "        jitter = rng.normal(loc=0.0, scale=noise, size=len(trace))\n",
    "        perturbed = trace.copy()\n",
    "        perturbed[\"throughput_kbps\"] = np.clip(perturbed[\"throughput_kbps\"] * (1 + jitter), 200.0, None)\n",
    "        perturbed[\"latency_ms\"] = np.clip(\n",
    "            perturbed[\"latency_ms\"] * (1 + rng.normal(0.0, noise / 2, len(trace))),\n",
    "            5.0,\n",
    "            None,\n",
    "        )\n",
    "        perturbed[\"loss_rate\"] = np.clip(\n",
    "            perturbed[\"loss_rate\"] * (1 + rng.normal(0.0, noise, len(trace))),\n",
    "            0.0,\n",
    "            1.0,\n",
    "        )\n",
    "        perturbed[\"buffer_level_s\"] = np.clip(\n",
    "            perturbed[\"buffer_level_s\"] * (1 + rng.normal(0.0, noise / 2, len(trace))),\n",
    "            0.0,\n",
    "            None,\n",
    "        )\n",
    "        perturbed[\"rebuffer_event\"] = (perturbed[\"buffer_level_s\"] <= 0.5).astype(float)\n",
    "        augmented.append(perturbed)\n",
    "    combined = pd.concat(augmented, ignore_index=True)\n",
    "    combined.sort_values(\"time_s\", inplace=True)\n",
    "    combined.reset_index(drop=True, inplace=True)\n",
    "    return combined\n",
    "\n",
    "\n",
    "def describe_client_traces(traces: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for client_id, df in traces.items():\n",
    "        rows.append(\n",
    "            {\n",
    "                \"client\": client_id,\n",
    "                \"scenario\": df[\"scenario\"].iloc[0],\n",
    "                \"samples\": len(df),\n",
    "                \"mean_throughput_kbps\": df[\"throughput_kbps\"].mean(),\n",
    "                \"mean_buffer_s\": df[\"buffer_level_s\"].mean(),\n",
    "                \"rebuffer_rate\": df[\"rebuffer_event\"].mean(),\n",
    "                \"mean_qoe\": df[\"qoe_score\"].mean(),\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows).sort_values(\"client\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9877eb",
   "metadata": {},
   "source": [
    "\n",
    "### Base Network Profiles\n",
    "\n",
    "We instantiate representative bandwidth patterns (home, rural, vehicular). Additional clients reuse these templates with stochastic noise to reach 10-15 participants when required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fff3de96",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SEED' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 41\u001b[0m\n\u001b[0;32m      1\u001b[0m BASE_SCENARIOS \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     NetworkScenario(\n\u001b[0;32m      3\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhome_stable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     ),\n\u001b[0;32m     35\u001b[0m ]\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_client_traces\u001b[39m(\n\u001b[0;32m     39\u001b[0m     num_clients: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m     40\u001b[0m     duration_s: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m600\u001b[39m,\n\u001b[1;32m---> 41\u001b[0m     base_seed: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mSEED\u001b[49m,\n\u001b[0;32m     42\u001b[0m     replay_factor: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     43\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, pd\u001b[38;5;241m.\u001b[39mDataFrame]:\n\u001b[0;32m     44\u001b[0m     rng \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mdefault_rng(base_seed)\n\u001b[0;32m     45\u001b[0m     traces: Dict[\u001b[38;5;28mstr\u001b[39m, pd\u001b[38;5;241m.\u001b[39mDataFrame] \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SEED' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "BASE_SCENARIOS = [\n",
    "    NetworkScenario(\n",
    "        name=\"home_stable\",\n",
    "        base_throughput_kbps=6500.0,\n",
    "        throughput_std=450.0,\n",
    "        playback_bitrate_kbps=4200.0,\n",
    "        base_latency_ms=32.0,\n",
    "        latency_std=6.0,\n",
    "        base_loss=0.008,\n",
    "        loss_std=0.003,\n",
    "        initial_buffer_s=24.0,\n",
    "    ),\n",
    "    NetworkScenario(\n",
    "        name=\"rural_unstable\",\n",
    "        base_throughput_kbps=2800.0,\n",
    "        throughput_std=900.0,\n",
    "        playback_bitrate_kbps=3600.0,\n",
    "        base_latency_ms=85.0,\n",
    "        latency_std=20.0,\n",
    "        base_loss=0.035,\n",
    "        loss_std=0.012,\n",
    "        initial_buffer_s=18.0,\n",
    "    ),\n",
    "    NetworkScenario(\n",
    "        name=\"vehicular_variable\",\n",
    "        base_throughput_kbps=4200.0,\n",
    "        throughput_std=1400.0,\n",
    "        playback_bitrate_kbps=3800.0,\n",
    "        base_latency_ms=115.0,\n",
    "        latency_std=28.0,\n",
    "        base_loss=0.022,\n",
    "        loss_std=0.009,\n",
    "        initial_buffer_s=14.0,\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "def generate_client_traces(\n",
    "    num_clients: int,\n",
    "    duration_s: int = 600,\n",
    "    base_seed: int = SEED,\n",
    "    replay_factor: int = 2,\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    rng = np.random.default_rng(base_seed)\n",
    "    traces: Dict[str, pd.DataFrame] = {}\n",
    "    for idx in range(num_clients):\n",
    "        base = BASE_SCENARIOS[idx % len(BASE_SCENARIOS)]\n",
    "        scenario_variant = NetworkScenario(\n",
    "            name=f\"{base.name}_v{idx + 1}\",\n",
    "            base_throughput_kbps=base.base_throughput_kbps * (1 + rng.normal(0.0, 0.08)),\n",
    "            throughput_std=base.throughput_std * (1 + rng.normal(0.0, 0.15)),\n",
    "            playback_bitrate_kbps=base.playback_bitrate_kbps * (1 + rng.normal(0.0, 0.05)),\n",
    "            base_latency_ms=base.base_latency_ms * (1 + rng.normal(0.0, 0.12)),\n",
    "            latency_std=max(5.0, base.latency_std * (1 + rng.normal(0.0, 0.2))),\n",
    "            base_loss=float(np.clip(base.base_loss * (1 + rng.normal(0.0, 0.25)), 0.002, 0.2)),\n",
    "            loss_std=float(np.clip(base.loss_std * (1 + rng.normal(0.0, 0.25)), 0.0005, 0.2)),\n",
    "            initial_buffer_s=max(4.0, base.initial_buffer_s * (1 + rng.normal(0.0, 0.1))),\n",
    "            rebuffer_threshold_s=base.rebuffer_threshold_s,\n",
    "        )\n",
    "        trace = simulate_network_trace(\n",
    "            scenario_variant,\n",
    "            duration_s=duration_s,\n",
    "            seed=base_seed + 17 * (idx + 1),\n",
    "        )\n",
    "        if replay_factor:\n",
    "            trace = enrich_with_replays(\n",
    "                trace,\n",
    "                repeats=replay_factor,\n",
    "                noise=0.07,\n",
    "                seed=base_seed + 31 * (idx + 1),\n",
    "            )\n",
    "        client_id = f\"client_{idx + 1:02d}\"\n",
    "        trace[\"client\"] = client_id\n",
    "        traces[client_id] = trace.reset_index(drop=True)\n",
    "    return traces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafead13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CLIENT_COUNT = 3\n",
    "client_traces = generate_client_traces(num_clients=CLIENT_COUNT, duration_s=600)\n",
    "combined_traces = pd.concat(client_traces.values(), ignore_index=True)\n",
    "client_summary = describe_client_traces(client_traces)\n",
    "client_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecc567e",
   "metadata": {},
   "source": [
    "\n",
    "### Scaling the Federation\n",
    "\n",
    "Adjust CLIENT_COUNT (e.g. 10 or 15) in the cell above to extend the federation. All downstream steps automatically use the updated client dictionaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65323882",
   "metadata": {},
   "source": [
    "\n",
    "### Visual Diagnostics\n",
    "\n",
    "Use the helper below to inspect how throughput, buffer level, latency, and QoE evolve per client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb7eddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_network_overview(\n",
    "    traces: Dict[str, pd.DataFrame],\n",
    "    metrics: List[str] | None = None,\n",
    "    max_points: int | None = 400,\n",
    "):\n",
    "    metrics = metrics or [\"throughput_kbps\", \"buffer_level_s\", \"latency_ms\", \"qoe_score\"]\n",
    "    num_clients = len(traces)\n",
    "    num_metrics = len(metrics)\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=num_metrics,\n",
    "        ncols=num_clients,\n",
    "        figsize=(4 * num_clients, 3 * num_metrics),\n",
    "        sharex=\"col\",\n",
    "    )\n",
    "    if num_clients == 1:\n",
    "        axes = np.array([[axes[row]] for row in range(num_metrics)])\n",
    "    for col_idx, (client_id, df) in enumerate(sorted(traces.items())):\n",
    "        subset = df if max_points is None else df.head(max_points)\n",
    "        for row_idx, metric in enumerate(metrics):\n",
    "            ax = axes[row_idx, col_idx]\n",
    "            ax.plot(subset[\"time_s\"], subset[metric])\n",
    "            if row_idx == 0:\n",
    "                ax.set_title(client_id)\n",
    "            if col_idx == 0:\n",
    "                ax.set_ylabel(metric)\n",
    "    fig.tight_layout()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6bde83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Uncomment to visualise the first 400 samples for each client\n",
    "# plot_network_overview(client_traces, max_points=400);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7134a279",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Data Preparation for Learning\n",
    "\n",
    "We split each client dataset into train/test partitions and prepare tensors for PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0961a0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FEATURE_COLUMNS = [\n",
    "    \"throughput_kbps\",\n",
    "    \"buffer_level_s\",\n",
    "    \"latency_ms\",\n",
    "    \"loss_rate\",\n",
    "    \"quality_ratio\",\n",
    "    \"rebuffer_event\",\n",
    "]\n",
    "TARGET_COLUMN = \"qoe_score\"\n",
    "\n",
    "client_datasets: Dict[str, Dict[str, np.ndarray]] = {}\n",
    "central_train_features = []\n",
    "central_train_targets = []\n",
    "central_test_features = []\n",
    "central_test_targets = []\n",
    "\n",
    "for client_id, frame in client_traces.items():\n",
    "    X = frame[FEATURE_COLUMNS]\n",
    "    y = frame[TARGET_COLUMN]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.2,\n",
    "        random_state=SEED,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    client_datasets[client_id] = {\n",
    "        \"X_train\": X_train.to_numpy(dtype=np.float32),\n",
    "        \"X_test\": X_test.to_numpy(dtype=np.float32),\n",
    "        \"y_train\": y_train.to_numpy(dtype=np.float32),\n",
    "        \"y_test\": y_test.to_numpy(dtype=np.float32),\n",
    "    }\n",
    "    central_train_features.append(X_train.to_numpy(dtype=np.float32))\n",
    "    central_train_targets.append(y_train.to_numpy(dtype=np.float32))\n",
    "    central_test_features.append(X_test.to_numpy(dtype=np.float32))\n",
    "    central_test_targets.append(y_test.to_numpy(dtype=np.float32))\n",
    "\n",
    "central_data = {\n",
    "    \"X_train\": np.vstack(central_train_features),\n",
    "    \"y_train\": np.concatenate(central_train_targets),\n",
    "    \"X_test\": np.vstack(central_test_features),\n",
    "    \"y_test\": np.concatenate(central_test_targets),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9df907",
   "metadata": {},
   "source": [
    "\n",
    "## 4. QoE Modelling Utilities\n",
    "\n",
    "The helpers below are shared by the centralized and federated pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5db69e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_loader(features: np.ndarray, targets: np.ndarray, batch_size: int, shuffle: bool) -> DataLoader:\n",
    "    features_tensor = torch.from_numpy(features).float()\n",
    "    targets_tensor = torch.from_numpy(targets).float().unsqueeze(1)\n",
    "    dataset = TensorDataset(features_tensor, targets_tensor)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "\n",
    "class QoEPredictor(nn.Module):\n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(features)\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    ") -> float:\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "    for features, target in loader:\n",
    "        features = features.to(DEVICE)\n",
    "        target = target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(features)\n",
    "        loss = criterion(predictions, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * features.size(0)\n",
    "        total_samples += features.size(0)\n",
    "    return running_loss / max(total_samples, 1)\n",
    "\n",
    "\n",
    "def evaluate_model(model: nn.Module, loader: DataLoader, criterion: nn.Module) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    predictions: List[np.ndarray] = []\n",
    "    targets: List[np.ndarray] = []\n",
    "    with torch.no_grad():\n",
    "        for features, target in loader:\n",
    "            features = features.to(DEVICE)\n",
    "            target = target.to(DEVICE)\n",
    "            outputs = model(features)\n",
    "            predictions.append(outputs.cpu().numpy())\n",
    "            targets.append(target.cpu().numpy())\n",
    "    preds = np.vstack(predictions)\n",
    "    trues = np.vstack(targets)\n",
    "    loss_value = criterion(torch.from_numpy(preds), torch.from_numpy(trues)).item()\n",
    "    mae_value = mean_absolute_error(trues, preds)\n",
    "    rmse_value = math.sqrt(mean_squared_error(trues, preds))\n",
    "    return {\"loss\": loss_value, \"mae\": mae_value, \"rmse\": rmse_value}\n",
    "\n",
    "\n",
    "def evaluate_clients(model: nn.Module, splits: Dict[str, Dict[str, np.ndarray]], batch_size: int = 256) -> pd.DataFrame:\n",
    "    criterion = nn.MSELoss()\n",
    "    rows = []\n",
    "    for client_id, split in sorted(splits.items()):\n",
    "        loader = make_loader(split[\"X_test\"], split[\"y_test\"], batch_size=batch_size, shuffle=False)\n",
    "        metrics = evaluate_model(model, loader, criterion)\n",
    "        metrics[\"client\"] = client_id\n",
    "        rows.append(metrics)\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ac4cd8",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1 Centralized Baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee8f205",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_centralized(\n",
    "    data: Dict[str, np.ndarray],\n",
    "    epochs: int = 30,\n",
    "    batch_size: int = 128,\n",
    "    lr: float = 1e-3,\n",
    ") -> tuple[nn.Module, pd.DataFrame]:\n",
    "    model = QoEPredictor(input_dim=data[\"X_train\"].shape[1]).to(DEVICE)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_loader = make_loader(data[\"X_train\"], data[\"y_train\"], batch_size=batch_size, shuffle=True)\n",
    "    val_loader = make_loader(data[\"X_test\"], data[\"y_test\"], batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    history: List[Dict[str, float]] = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "        eval_metrics = evaluate_model(model, val_loader, criterion)\n",
    "        eval_metrics.update({\"epoch\": epoch, \"train_loss\": train_loss})\n",
    "        history.append(eval_metrics)\n",
    "    return model, pd.DataFrame(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffd48bf",
   "metadata": {},
   "source": [
    "\n",
    "### 4.2 Federated Training (FedAvg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823fd54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def federated_train(\n",
    "    splits: Dict[str, Dict[str, np.ndarray]],\n",
    "    rounds: int = 20,\n",
    "    local_epochs: int = 2,\n",
    "    batch_size: int = 64,\n",
    "    lr: float = 5e-4,\n",
    ") -> tuple[nn.Module, pd.DataFrame]:\n",
    "    first_client = next(iter(splits.values()))\n",
    "    model = QoEPredictor(input_dim=first_client[\"X_train\"].shape[1]).to(DEVICE)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    global_test_features = np.vstack([split[\"X_test\"] for split in splits.values()])\n",
    "    global_test_targets = np.concatenate([split[\"y_test\"] for split in splits.values()])\n",
    "    global_test_loader = make_loader(global_test_features, global_test_targets, batch_size=256, shuffle=False)\n",
    "\n",
    "    history: List[Dict[str, float]] = []\n",
    "\n",
    "    for round_idx in range(1, rounds + 1):\n",
    "        aggregated_state = {key: torch.zeros_like(val) for key, val in model.state_dict().items()}\n",
    "        total_samples = 0\n",
    "        local_states = []\n",
    "        local_weights = []\n",
    "\n",
    "        for client_id, split in sorted(splits.items()):\n",
    "            local_model = QoEPredictor(input_dim=first_client[\"X_train\"].shape[1]).to(DEVICE)\n",
    "            local_model.load_state_dict(model.state_dict())\n",
    "            optimizer = torch.optim.Adam(local_model.parameters(), lr=lr)\n",
    "            train_loader = make_loader(split[\"X_train\"], split[\"y_train\"], batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            for _ in range(local_epochs):\n",
    "                train_one_epoch(local_model, train_loader, optimizer, criterion)\n",
    "\n",
    "            local_state = {key: value.detach().clone() for key, value in local_model.state_dict().items()}\n",
    "            sample_count = len(split[\"X_train\"])\n",
    "            local_states.append(local_state)\n",
    "            local_weights.append(sample_count)\n",
    "            total_samples += sample_count\n",
    "\n",
    "        for state, weight in zip(local_states, local_weights):\n",
    "            coeff = weight / total_samples\n",
    "            for key in aggregated_state:\n",
    "                aggregated_state[key] = aggregated_state[key] + state[key] * coeff\n",
    "\n",
    "        model.load_state_dict(aggregated_state)\n",
    "        metrics = evaluate_model(model, global_test_loader, criterion)\n",
    "        metrics.update({\"round\": round_idx})\n",
    "        history.append(metrics)\n",
    "\n",
    "    return model, pd.DataFrame(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f08683",
   "metadata": {},
   "source": [
    "\n",
    "### 4.3 Unified Experiment Driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d5a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_experiment(\n",
    "    client_splits: Dict[str, Dict[str, np.ndarray]],\n",
    "    central_data: Dict[str, np.ndarray],\n",
    "    central_epochs: int = 30,\n",
    "    central_lr: float = 1e-3,\n",
    "    central_batch: int = 128,\n",
    "    fed_rounds: int = 25,\n",
    "    fed_local_epochs: int = 2,\n",
    "    fed_lr: float = 5e-4,\n",
    "    fed_batch: int = 64,\n",
    ") -> Dict[str, object]:\n",
    "    central_model, central_history = train_centralized(\n",
    "        central_data,\n",
    "        epochs=central_epochs,\n",
    "        batch_size=central_batch,\n",
    "        lr=central_lr,\n",
    "    )\n",
    "    fed_model, fed_history = federated_train(\n",
    "        client_splits,\n",
    "        rounds=fed_rounds,\n",
    "        local_epochs=fed_local_epochs,\n",
    "        batch_size=fed_batch,\n",
    "        lr=fed_lr,\n",
    "    )\n",
    "    central_eval = evaluate_clients(central_model, client_splits)\n",
    "    fed_eval = evaluate_clients(fed_model, client_splits)\n",
    "\n",
    "    comparison = central_eval.merge(fed_eval, on=\"client\", suffixes=(\"_central\", \"_federated\"))\n",
    "\n",
    "    return {\n",
    "        \"central_model\": central_model,\n",
    "        \"central_history\": central_history,\n",
    "        \"federated_model\": fed_model,\n",
    "        \"federated_history\": fed_history,\n",
    "        \"comparison\": comparison,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b959c4",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Experimentation & Analysis\n",
    "\n",
    "The snippets below illustrate how to launch baseline comparisons and sensitivity studies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57c2914",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: baseline comparison for the current CLIENT_COUNT\n",
    "# results = run_experiment(\n",
    "#     client_splits=client_datasets,\n",
    "#     central_data=central_data,\n",
    "#     central_epochs=25,\n",
    "#     fed_rounds=20,\n",
    "#     fed_local_epochs=2,\n",
    "# )\n",
    "# display(results[\"comparison\"])\n",
    "# ax = results[\"federated_history\"].plot(x=\"round\", y=[\"loss\", \"mae\", \"rmse\"], marker=\"o\");\n",
    "# ax.set_title(\"Federated metrics per round\");\n",
    "# ax.set_ylabel(\"Score\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6b83d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_federated_configs(\n",
    "    client_splits: Dict[str, Dict[str, np.ndarray]],\n",
    "    configs: List[Dict[str, int | float | str]],\n",
    ") -> pd.DataFrame:\n",
    "    records = []\n",
    "    for cfg in configs:\n",
    "        label = cfg.get(\"label\") or f\"rounds={cfg['rounds']}, local_epochs={cfg['local_epochs']}\"\n",
    "        _, history = federated_train(\n",
    "            client_splits,\n",
    "            rounds=int(cfg.get(\"rounds\", 20)),\n",
    "            local_epochs=int(cfg.get(\"local_epochs\", 2)),\n",
    "            batch_size=int(cfg.get(\"batch_size\", 64)),\n",
    "            lr=float(cfg.get(\"lr\", 5e-4)),\n",
    "        )\n",
    "        last_metrics = history.iloc[-1].to_dict()\n",
    "        last_metrics[\"configuration\"] = label\n",
    "        records.append(last_metrics)\n",
    "    return pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef7225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: study the impact of update frequency\n",
    "# configs = [\n",
    "#     {\"label\": \"Fast updates\", \"rounds\": 15, \"local_epochs\": 1},\n",
    "#     {\"label\": \"Balanced\", \"rounds\": 20, \"local_epochs\": 2},\n",
    "#     {\"label\": \"Delayed\", \"rounds\": 10, \"local_epochs\": 4},\n",
    "# ]\n",
    "# compare_federated_configs(client_datasets, configs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b584ff9",
   "metadata": {},
   "source": [
    "\n",
    "### Next Steps\n",
    "\n",
    "- Increase CLIENT_COUNT to 10 or 15 and rerun the workflow to assess scalability.\n",
    "- Try alternative aggregation rules (e.g. FedProx) by adjusting \f\n",
    "ederated_train.\n",
    "- Log additional QoS indicators (stall duration, bitrate switches) to enrich the QoE target.\n",
    "- Persist experiment artefacts (histories, comparisons) to CSV for offline analysis when needed.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comyco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
